# <center> Papers about Vision Language Models</center>

This repo lists recent advantages on VLMs, mainly contributed by [Weihan Wang](https://github.com/mactavish91) and [Ji Qi](https://github.com/qijimrc).

## Papers



| Model            | Vision Enc.          | Textual Enc.       | Dec.    | Pretraining Objectives  | Exp. Tasks                 | Published Year |   
|:----------------:|:--------------------:|:------------------:|:-------:|:-----------------------:|:--------------------------:|:--------------:|
|  [ViLBERT](https://proceedings.neurips.cc/paper/2019/file/c74d97b01eae257e44aa9d5bade97baf-Paper.pdf)  | OD(*I*)->Xformer     |  *T*->Xformer      |    /    | MLM + ITM + MIM         |                            |  2019 (NIPS)   |
|          |   |   |   |   |   |   |


#### Notes:
 - *I*            : image inputs
 - *T*            : text inputs
 - **OD**         : objective detector
 - **Xformer**    : transformer
 - **Emb.**       : embedding
 - **MLM**        : masked language modeling
 - **MIM**        : masked image modeling
 - **ITM**        : image-text matching
 - **WRA**        : word-region alignment
 - **ITC**        : image-text contrastive learning
