# <center> Papers about Vision Language Models</center>

This repo lists recent advantages on Vision Language Models (VLMs), mainly contributed by [Ji Qi](https://github.com/qijimrc) and [Weihan Wang](https://github.com/weihang-wang).

## Papers



| Model            | Vision Enc.          | Textual Enc.       | Dec. | Pretraining Objectives | Exp. Tasks                 | Published Year |   
|:----------------:|:--------------------:|:------------------:|:-------:|:-----------------------:|:----------------------:|:--------------:|
|  [ViLBERT](https://proceedings.neurips.cc/paper/2019/file/c74d97b01eae257e44aa9d5bade97baf-Paper.pdf)  | OD(*I*)->Xformer       |  *T*->Xformer        |    /    | MLM + ITM + MIM         |                        |  2019 (NIPS)   |
|          |   |   |   |   |   |   |


#### Notes:
 - *I*            : image inputs
 - *T*            : text inputs
 - **OD**         : objective detector
 - **Xformer**    : transformer
 - **Emb.**       : embedding
 - **MLM**        : masked language modeling
 - **MIM**        : masked image modeling
 - **ITM**        : image-text matching
 - **WRA**        : word-region alignment
 - **ITC**        : image-text contrastive learning
